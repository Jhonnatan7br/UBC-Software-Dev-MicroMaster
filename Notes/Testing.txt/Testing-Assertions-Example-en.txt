
REID HOLMES: So let's step back and take a concrete look
at how we can actually use assertions to validate
that a program is correct in practice.
So what we're going to do here is we're going
to step through a concrete example of a survey system.
So let's take a look at the high-level interfaces for this system.
So here we have survey question, which maintains an index number, a question,
and then an answer to that question.
And we have a multiple choice question, which
extends survey question to add a little bit more options for multiple choice
questions.
And here's our main class that we need to implement-- survey.
And survey maintains this list of questions and this pointer
to the current question.
And what we want to do is you want to step through and make sure
that all of this code is actually correct to ensure
that it does what the specification says it's supposed to do.
And looking at the structure of this class, there really isn't a lot to it.
There's these two properties that we talked about before, a constructor,
and a series of methods.
So we just want to step through and make sure all of this works correctly.
Let's start by creating the test for this system, OK?
So let's make a new file called SurveySpec.ts.
In the behavioral-driven development world, tests are often called specs,
so we're just going to call this survey Spec.
So first thing we need to do is to actually import the Mocha library,
because that's what we're going to be doing
to actually provide the mechanism for performing these tests.
And next we need to go and start to describe the test that we
want to do and create the test cases.
So in the BDD world, we talked earlier about having this Given-When-Then style
test.
All right, so here we say, describe the basic test for survey.
That's the given, right?
Our basic test.
And now we want to go and create individual test cases
that correspond to the when clause.
These it clauses correspond to our individual test cases.
So one of the first things we might want to test for this survey system
is that we should be able to get to the next question.
And that's what we're going to test here.
So given a basic survey system, we should
be able to get to the next question.
That's really what we're trying to test.
Before we can actually do anything, what we need
is some surveys to actually run some tests against.
So that's what we're going to do next.
So essentially before any test case, what we want to do
is we want to create a survey that we can then go and validate.
And we want to do this for every test case.
Because as we manipulate these object instances in the test itself,
we're going to be changing their internal state.
So we're going to use this special feature called beforeEach.
And essentially in this beforeEach function,
what we're going to do is we're going to set up a survey,
s, with some members and a second list of members.
And before each test we'll actually go and repopulate
these three fields with fresh new values.
So we can make sure that the tests don't interact with one other.
Because tests should really be independent,
and this is one way to check for that for sure.
We also need to make sure that we import the types that we're actually
going to be validating.
So that's what we're going to do here.
We'll import the survey, and we'll import multiple choice answer
and survey question from their respective files.
And what we've done here is just pasted in some initialization
code for members and members 2, so that these fields are available for testing.
So they'll be repopulated on every single test case that gets run.
Ultimately, in every single test we want to check some actual behavior
against an expected behavior.
So here we've gone and declared a variable called actual
that's the response to ask.getQuestion.
And then our expected value, what we expect
the answer to be, we've set it to an explicit variable called expected.
And this makes reading the assertion really natural.
We can say, "expect actual to equal expected."
And this makes it really easy to understand
what's going on in this test.
Oops, it looks like I forgot to import expect itself.
So we had a compilation error.
But we can go and ignore that.
Right, it's pretty straightforward to fix.
OK, so now we have an actual test case here.
And let's see what happens when we run it.
All right, so when we run this, we can see
that the test returns a green value, right?
This means it worked.
But what if it just passed by accident?
What if the test wasn't running correctly?
So let's pass in a fake value here, just to make sure that it fails.
It's always a nice idea to do a sanity check
to make sure that your tests have the opportunity
to fail, or can possibly fail, before we can consider them finished.
All right, so let's go through and try to extend this test case a little bit
further.
One thing you might want to do is you might
want to make sure that a property that you don't expect a return
object to have isn't actually present.
So here we're going to go and check a negative expectation.
So here we say, expert actual to not have property right answer.
Right, so this is no prop here.
And we want to be able to run this and make sure that this property doesn't
actually exist in the output.
And since we're checking that some random property doesn't exist,
maybe we should go through and make sure that all the right properties do exist.
So here we could say, all keys.
Here are all the keys that we expect actual to have.
So we expect there to be num, answer, and question.
And what we want to do is we want to make sure that all of those properties
actually exist.
And we can do this really naturally, too.
We can say expect actual to have all keys.
All keys.
And what that means is that all of those keys
are present in the returned object.
And then we can go and run this test.
And again, we see that it passes, which is great.
It's exactly what we want to happen.
When we talked about white box testing earlier,
we said that measuring the actual lines of code that are executed by a test
is a good way to see what portions of the system
have been run by a test suite.
So here we can just run NPM run cover, and take a look
at what the results end up being.
One handy thing about most coverage tools is,
they'll actually generate an HTML report that you can use to see and understand
what parts of your system have been run by a test,
and what parts of your system haven't.
And when we look at this report, we see that getNextQuestion isn't actually
being tested by our existing test suite, nor is getAllQuestions.
Now it would be really tempting to go and write
tests for getNextQuestion and getAllQuestions,
because we can see that they're uncovered in the coverage report.
But looking at the getQuestions API specification,
I actually think that we haven't tested this function sufficiently enough.
Because when you look at the expectation for this method, what the spec says,
there's an extra case that should happen here
if current question does not exist.
There should be an error thrown.
And we definitely don't test this.
So before we get distracted by the coverage,
let's take a look at writing another test to make sure
that getQuestion is doing the right thing.
So here we're going to write a test that checks
to make sure that an error is indeed thrown when a question does not exist.
Since our local survey that's set up for each method
has some questions associated with it, we'll
actually create a new survey here that we know doesn't have any questions.
Because really, in this case, we're trying
to make sure that we get an error when no questions are being set
and we asked for the next one.
One thing that's going to be a little bit weird here,
too, is to check for exceptions being thrown or not.
We need to use this funny syntax.
So we'll go and step through this here.
This is using fat arrow notation.
So here we'll create this inner function called getActual,
and we'll call test.getQuestion from within it.
And here we can actually write our expectation.
We can say expect getActual to throw--
and we've provided with the error message that we expect to see.
And in this case, that's no questions.
Now we had to use this fat arrow notation
so that we were able to actually capture the exception that was thrown.
That's a little bit less natural than the expectations we had earlier.
But it still works pretty well.
Right, so let's run this test and see what happens.
So let's give a run here.
And when we run the test, we actually see that it fails, right?
And why is that?
It says expect null to be undefined.
What's going on there?
If current question does not exist, there should be an error.
But when we look at the implementation here,
we can see that the implementation for getQuestions isn't actually right.
So let's go and take a look at how we can fix this
so that we can make the test pass.
Because this is definitely a bug.
So let's go and write a proper implementation for this.
So here we could say, hey, let val equals the current question that we
expect to have.
And then we want to make sure that val isn't undefined, right?
And if val isn't undefined, we'll return it.
Because that's the right question that we want to return.
And if it doesn't, we want to say, hey, throw an error
with this message, "no questions."
Now when we go and we run the tests again,
here we will see that the test actually passes.
And that's perfect.
That means we fixed the bug.
And the implantation actually meets the spec now.
So by going through this process of adding a new test,
we found a bug in our implementation and we were able to fix it.
And let's just go and take a quick peek at the coverage for getQuestion now.
So here we can see when we look at it that even though the code got bigger,
our tests actually managed to cover both halves of this function.
So we feel like we're covering this pretty well.
And having read through this spec again, as we wrote the tests,
we could feel pretty good about the tests
that we've written for getQuestionNow.
So let's move on to getNextQuestion, which
we identified earlier as being something we need
to look at from the coverage report.
OK, so let's make sure that this iterator works.
So we'll say again, we have actual, which is getNextQuestion.
This is the result of our operation.
And then we want to specify our expected value.
So again, we'll set another local variable
to be expected, which is member's one.
The default should start on member 0.
So after we iterate, we should be on member's one.
Let's just write our basic expectation here.
Expect actual to equal expected.
And then we can go and run the test and make sure that it passes.
And here we can see that it does pass.
So this implementation is right.
So let's take a look at the coverage report from the last run.
And here we can see, getNextQuestion isn't actually run.
This is before we've regenerated the coverage report.
So let's run the coverage report again, and take another look
at the coverage to see what happens.
And here we can say, yeah, look, we now cover this case, right?
This is exactly what we expect to happen.
But oh, look, we don't cover this else clause.
We need to check this no next question error as well.

All right, so we want to check this error case for this function,
just like we checked it with the last one.
So we'll go and create a new test case here.
Now since this looks really similar to our prior test case, what we're just
going to do is copy the old test to the new one, and then adapt it as required,
right?
So instead of calling getQuestion, we're going to call getNextQuestion.
And then the error message is also slightly different.
So we'll go and check what that is supposed to be.
And the answer is no next question instead of no questions.
All right, so we'll go and adapt the string here, as well.
And now we can run the test and see what happens.

And as we were hoping, the test passes.
So that's great.
So let's just go and quickly regenerate the coverage report, just as
a sanity check to make sure that that block is now covered.
So here we were missing the error block before.
And when we refresh, we can see that the error block is covered.
So that's perfect. getNextQuestion also seems to be tested fairly adequately.
Now we've mostly been talking about checking keys and checking equality.
But sometimes you want to check properties
that are a little bit more challenging to validate in practice.
So let's take a specific example for a numerical type, here.
And what we're going to do is we're going to go and compare
the value of a division operation.
So here we'll say, dividing 22 by 7.
And just try to evaluate what it actually equals to.
So we'll go and set up our survey and answer a question with this 22 over 7.
And we'll take a look at performing an expectation on this.
Now 22 over 7 is approximately pi.
So we might say, expect the answer to equal 3.14, right?
To equal pi.

Now when we go and run this test, what's interesting
is that the test will actually fail.
Because it says hey, look, 3.14 doesn't equal 3.142, right?
It's not the same value.
So we need to look at doing this in a different way.
So for a numerical type like this, we can just say hey,
we want to expect the answer, q.answer, to be within a certain tolerance,
within a bounds.
So we're just going to say, we want it to be above 3.141 and below 3.143.
And then we can go and run this test again and take a look at the output.
And in this case, we can see that the test actually passes.
Right, so this works.
So this provides a more natural way for us
to test that a numerical type is within a tolerance that we care about.
All right, so let's run the coverage report now and see where we stand.
OK, so take a look.
So open up the view here.
We scroll down and we say, oh yeah, we're missing getAllQuestions.
There's a few little blocks here and there.
But these are pretty big blocks that we don't have.
So if we're going to cover something, we might as well
cover something big like this.
So let's take a look at this function here
and write a test that works for this.
Now, one thing to note here that this is an asynchronous function.
It's returning a promise.
So this is going to make our testing process a little bit trickier.
So let's talk a little bit about what that looks like in practice.
So here we go, just like before, we'll go and set up our it clause that
will actually encompass our test.
And we can start writing our function here.
Even though this is a asynchronous function,
let's just go and try and do it the straightforward way,
as if it was synchronous.
So let's just go and say s.loadQuestionsFromFile.
We have a sample file here called sample.txt
that we know contains three questions.
And we'll go and load that up and make this work.
We have to be careful here, because we don't actually
want to load questions from file into s, into our local survey.
We want to create a new survey instance here within this test,
so that we're not using the ones that have
been pre-populated with other values.
So we're going to have to go and create a new survey instance first.
And then what we want to go and do is check to make sure it actually works.
So let's go and call test.getAllQuestions.
Let's get all the questions and check its length.
So our actual value is going to be the length of the number of questions
that have been returned.
And because we know that the sample file has three questions in it,
we'll just hard code 3 as the expected length of this array.
And then we could just expect that actual equals expected,
just like we've been using previously.
And when we go and we run this test, we're
actually going to see that it fails.
And this is kind of weird, right?
Actual length is 0, and the expected is 3.
So what's actually going on here?
This is probably an asynchronous problem that's causing us pain.
And what's really happening is that we're checking the length of the array
before it's actually done loading.
If we treat it synchronously, we're not going
to wait for the loading to finish before we do that expectation.
So we need to do this in a different way.
Let's promise-ify this test now.
So one thing that we're doing here that's really important,
is you can see that we're returning the promise in this call, here.
When we return the promise, it tells Mocha
that this is actually a promise-ified function.
So it will wait for it to finish, either to invoke the then clause
or to invoke the catch.
Let's go ahead and move our previous check up
into within the body of this then clause.
And when we go and we run this test, we can see that it actually passes.
So that's perfect, that's doing exactly what we want.
And if we go back and check the coverage report now,
we'll have to regenerate it of course.
We regenerate the coverage report and take a look.
We can see that this function is now covered.
And of course, there's this clause that we've missed.
And if we wanted to go and add more tests to the rest of this class
to make this test suite complete, we'd have
to go back and fill in the rest of these uncovered bits of code.
So there's an error case here, and there's a few other individual lines
of code that need to be looked at.
Hopefully this concrete example has given you
some understanding of the process that's involved in creating tests for a file.
You don't think of everything up front.
You go and you iterate through the tests one at a time.
And some idea of some of the mechanical operations that you'll also go through,
in terms of running the tests, making sure they both pass and fail,
and checking the coverage reports so that you can do this effectively
in the future.